{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "a9b5fa65",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "a9b5fa65"
      },
      "outputs": [],
      "source": [
        "# Backward Selection\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_selection import SequentialFeatureSelector\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "4e080b2b",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "4e080b2b"
      },
      "outputs": [],
      "source": [
        "# Load data\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "X = pd.DataFrame(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "7b38155f",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "7b38155f"
      },
      "outputs": [],
      "source": [
        "# Split dataset into train and test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "a028b5d0",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "a028b5d0"
      },
      "outputs": [],
      "source": [
        "# Create logistic regression model\n",
        "model = LogisticRegression(max_iter=5000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cca6f470",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "cca6f470"
      },
      "outputs": [],
      "source": [
        "# Backward Selection: start with all features and remove the least significant one by one\n",
        "sfs_backward = SequentialFeatureSelector(\n",
        "    model, n_features_to_select=5, direction='backward'\n",
        ")\n",
        "sfs_backward.fit(X_train, y_train)\n",
        "\n",
        "selected_features = X.columns[sfs_backward.get_support()]\n",
        "print(\"Backward Selection Chosen Features:\", list(selected_features))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a2946ba1",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "a2946ba1"
      },
      "outputs": [],
      "source": [
        "# Train model with selected features and evaluate\n",
        "model.fit(X_train[selected_features], y_train)\n",
        "y_pred = model.predict(X_test[selected_features])\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy with Backward Selection (5 features): {acc:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "30345432",
      "metadata": {
        "id": "30345432"
      },
      "source": [
        "## Analysis:\n",
        "- Forward selection generally starts from an empty set and adds features that best improve\n",
        "- model performance incrementally. This can help identify a small subset of highly predictive features.\n",
        "- You may observe slightly improved or comparable accuracy to using all features with fewer variables,indicating a good feature subset."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "14535568",
      "metadata": {
        "id": "14535568"
      },
      "source": [
        "## 1. Automated Feature‐Count Tuning\n",
        "#### Adapt your script to automatically select the optimal n_features_to_select from a range (e.g., 1–30) by choosing the value that maximizes test accuracy.\n",
        "\n",
        "- Loop over possible feature counts.\n",
        "\n",
        "- Record test accuracy for each.\n",
        "\n",
        "- Print the best feature count and its accuracy.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "99eb3fb4",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "99eb3fb4"
      },
      "outputs": [],
      "source": [
        "best_k, best_acc = None, 0\n",
        "for k in range(1, 31):\n",
        "    sfs = SequentialFeatureSelector(\n",
        "        model, n_features_to_select=k, direction='backward'\n",
        "    ).fit(X_train, y_train)\n",
        "    feats = X_train.columns[sfs.get_support()]\n",
        "    model.fit(X_train[feats], y_train)\n",
        "    acc = accuracy_score(y_test, model.predict(X_test[feats]))\n",
        "    if acc > best_acc:\n",
        "        best_k, best_acc = k, acc\n",
        "\n",
        "print(f\"Best feature count: {best_k} → Accuracy: {best_acc:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b415b16f",
      "metadata": {
        "id": "b415b16f"
      },
      "source": [
        "## 2. Stratified K-Fold Integration\n",
        "#### Replace the single train/test split with Stratified K-Fold during feature selection:\n",
        "\n",
        "- Use StratifiedKFold(n_splits=5) within SequentialFeatureSelector.\n",
        "\n",
        "- Report mean and standard deviation of accuracy across folds for your final feature set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d124334",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "9d124334"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "\n",
        "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "sfs = SequentialFeatureSelector(\n",
        "    model,\n",
        "    n_features_to_select=5,\n",
        "    direction='backward',\n",
        "    scoring='accuracy',\n",
        "    cv=kf\n",
        ").fit(X, y)\n",
        "\n",
        "feats = X.columns[sfs.get_support()]\n",
        "scores = cross_val_score(model, X[feats], y, cv=kf)\n",
        "print(\"Selected Features:\", list(feats))\n",
        "print(f\"CV Accuracy: {scores.mean():.4f} ± {scores.std():.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "83f80ea5",
      "metadata": {
        "id": "83f80ea5"
      },
      "source": [
        "## 3. Custom Scoring Function\n",
        "#### Experiment with a different scoring metric in forward selection (e.g., F1-score or balanced accuracy) to handle class imbalance:\n",
        "\n",
        "- Pass scoring='f1' or scoring='balanced_accuracy' to SequentialFeatureSelector.\n",
        "\n",
        "- Compare the selected feature lists and test performance under each metric."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33d11240",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "33d11240"
      },
      "outputs": [],
      "source": [
        "sfs_bal = SequentialFeatureSelector(\n",
        "    model,\n",
        "    n_features_to_select=5,\n",
        "    direction='backward',\n",
        "    scoring='balanced_accuracy'\n",
        ")\n",
        "sfs_bal.fit(X_train, y_train)\n",
        "feats_bal = X_train.columns[sfs_bal.get_support()]\n",
        "model.fit(X_train[feats_bal], y_train)\n",
        "print(\"Balanced Accuracy Features:\", list(feats_bal))\n",
        "\n",
        "from sklearn.metrics import balanced_accuracy_score\n",
        "bal_acc = balanced_accuracy_score(y_test, model.predict(X_test[feats_bal]))\n",
        "print(f\"Balanced Accuracy on Test Set: {bal_acc:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ccfd4cec",
      "metadata": {
        "id": "ccfd4cec"
      },
      "source": [
        "## 4. Application to a New Dataset\n",
        "#### Apply your backward‐selection pipeline unchanged to a different classification dataset (e.g., the Iris or Wine dataset):\n",
        "\n",
        "- Load a new dataset from sklearn.datasets.\n",
        "\n",
        "- Compare which features are selected and the resulting model accuracy.\n",
        "\n",
        "- Each exercise reuses your original code structure and deepens your grasp of forward wrapper selection through tuning, validation, stability, and computational considerations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa8777be",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "aa8777be"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_wine\n",
        "\n",
        "X_wine, y_wine = load_wine(return_X_y=True, as_frame=True)\n",
        "X_tr, X_ts, y_tr, y_ts = train_test_split(X_wine, y_wine, random_state=42)\n",
        "\n",
        "sfs_wine = SequentialFeatureSelector(\n",
        "    model, n_features_to_select=5, direction='backward'\n",
        ").fit(X_tr, y_tr)\n",
        "\n",
        "feats = X_tr.columns[sfs_wine.get_support()]\n",
        "model.fit(X_tr[feats], y_tr)\n",
        "acc = accuracy_score(y_ts, model.predict(X_ts[feats]))\n",
        "\n",
        "print(f\"Backward Features (Wine): {list(feats)}\")\n",
        "print(f\"Wine Dataset Accuracy: {acc:.4f}\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}